{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indoor-atlanta",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "\n",
    "This notebook describes technical elements used in concert for this work. \n",
    "\n",
    "\n",
    "\n",
    "In summary: \n",
    "\n",
    "\n",
    "\n",
    "- bash, text editor, git, GitHub\n",
    "- running a Jupyter notebook server (code and markdown)\n",
    "- Pulling datasets from OOI\n",
    "- Basic Python as the baseline layer of the code\n",
    "    - Start by putting all code in notebooks\n",
    "    - Be prepared to migrate some of the perfunctory code to a module file\n",
    "- Install Python data science libraries for plotting, opening datasets, slicing\n",
    "    - starting with `matplotlib`, `numpy`, `pandas`, and `xarray`\n",
    "- As needed bring in Python extension libraries\n",
    "    - interactive widgets, maps, animation, color maps\n",
    "- Pulling data from the OOI data system\n",
    "- Pulling datasets from other programs: ARGO, MODIS, GLODAP, ROMS, MSLA, etcetera\n",
    "- Reducing code-to-visualization ratio using Python module structure\n",
    "- Binder as ephemeral sandbox\n",
    "- Working from larger extra-repo datasets\n",
    "- Deconstructing arcane elements of the analysis\n",
    "    - NetCDF-to-xarray-Dataset\n",
    "    - Subsets of datasets\n",
    "    - Profiler ascent/descent/rest characterization\n",
    "    \n",
    "    \n",
    "    \n",
    "Using a first-person narrative let's describe getting started...\n",
    "\n",
    "\n",
    "- I learn the basic commands of the `bash` shell; including how to use a text editor like `nano` or `vim`\n",
    "- I create an account at `github.com` and learn to use the basic `git` commands\n",
    "    - `git pull`, `git add`, `git commit`, `git push`, `git clone`, `git stash`\n",
    "    - I plan to spend a couple of hours learning `git`; I find good YouTube tutorials\n",
    "- I create my own GitHub repository with a `README.md` file describing my research goals\n",
    "- I set up a Jupyter notebook server on my local machine\n",
    "    - As I am using a PC I install WSL-2 (Windows Subsystem for Linux v2)...\n",
    "        - ...and install Miniconda plus some Python libraries\n",
    "- I clone my \"empty\" repository from GitHub to my local Linux environment\n",
    "- I start my Jupyter notebook server, navigate to my repo, and create a first notebook\n",
    "- I save my notebook and use `git add, commit, push` to save it safely on GitHub\n",
    "- On GitHub: Add and test a **`binder`** badge\n",
    "    - Once that works, be sure to `git pull` the modified GitHub repo back into the local copy\n",
    "\n",
    "\n",
    "\n",
    "At this point we do not have any data; so let's do that next. There are two important considerations. \n",
    "First: If the data volume will exceed 100MB: That is too much to keep in a GitHub repository. The\n",
    "data must be staged \"nearby\" in the local environment; outside the repository but accessible by\n",
    "the repository code, as in:\n",
    "\n",
    "\n",
    "```\n",
    "               ------------- /repo directory\n",
    "              /\n",
    "/home --------\n",
    "              \\\n",
    "               -------------- /data directory\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Second: Suppose the repo *does* contain (smaller) datasets, to be read by the code. \n",
    "If the intent is to use `binder` to make a sandbox version of the repo\n",
    "available, all significant changes to this code should be tested: First locally\n",
    "and then (after a `push` to GitHub) ***in `binder`***. This ensures that not too \n",
    "many changes pile up, breaking binder in mysterious and hard-to-debug ways.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now that we have a dataset let's open it up and examine it within a Notebook.\n",
    "The data are presumed to be in NetCDF format; so we follow common practice of\n",
    "reading the data into an `xarray Dataset` which is a composition of `xarray\n",
    "DataArrays`. There is a certain amount of learning here, particularly as this\n",
    "library shares some Python DNA with `pandas` and `numpy`. Deconstructing an\n",
    "`xarray Dataset` can be very challenging; so a certain amount of ink is devoted\n",
    "to that process in this repo.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's make a time subset of the dataset and plot the data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's focus on a profiler.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's animate a time series.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Resources\n",
    "\n",
    "\n",
    "These are extensive notes on process, open tasks, challenges, workarounds. \n",
    "The Python Data Science Handbook is abbreviated herein as PDSH.\n",
    "\n",
    "\n",
    "\n",
    "## On using this notebook\n",
    "\n",
    "\n",
    "This notebook gets down into the weeds, the details of making this software work as intended.  \n",
    "Eventually it will be broken into multiple chapters under Documentation.\n",
    "I am using header size carefully so scroll or link on down until the header gets big again at\n",
    "the next topic. To help here is a TOC.\n",
    "\n",
    "* [Markdown](#Markdown)\n",
    "* [Reducing Datasets](#Reducing-Datasets)\n",
    "    * [One minute resampling](#One-minute-resampling)\n",
    "* [Plotting](#Plotting)\n",
    "    * [Making animations](#Making-animations)\n",
    "* [Multimedia](#Multimedia)\n",
    "    * [Images](#Images)\n",
    "    * [Animations](#Animations)\n",
    "    * [YouTube video playback](#YouTube-video-playback)\n",
    "    * [Sound clips](#Sound-clips)\n",
    "* [XArray Datasets and DataArrays](#XArray-Datasets-and-DataArrays)\n",
    "* [Pandas Series and DataFrames](#Pandas-Series-and-DataFrames)\n",
    "    * [Selecting based on a range](#Selecting-based-on-a-range)\n",
    "* [Numpy ndarrays](#Numpy-ndarrays)\n",
    "* [Time](#Time)\n",
    "* [ipywidgets](#ipywidgets)\n",
    "* [HoloView](#Holoview)\n",
    "* [Where I Left Off](#process-these-notes)\n",
    "* [Binder](#Binder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4c5faa",
   "metadata": {},
   "source": [
    "## Markdown\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### LaTeX math formulas \n",
    "\n",
    "These can be placed inline by means of a single dollar-sign: $e^x = \\sum_{i=0}^{\\infty}{\\frac{x^i}{i!}}$. \n",
    "Use double dollar-signs to create a centered equation that is not inlined. \n",
    "\n",
    "\n",
    "$$e^x = \\sum_{i=0}^{\\infty}{\\frac{x^i}{i!}}$$\n",
    "\n",
    "\n",
    "If centered is not ideal: Revert to single-dollar-sign on a blank line to left-justify. \n",
    "The down side is that single-dollar-sign tries to be vertically economical so it doesn't look as cool, \n",
    "at least by default. \n",
    "\n",
    "\n",
    "$e^x = \\sum_{i=0}^{\\infty}{\\frac{x^i}{i!}}$\n",
    "\n",
    "\n",
    "Anyway there is a wealth of LaTeX documentation out there; and in my experience most of it (but not necessarily\n",
    "all) maps to Jupyter markdown. LaTeX is so well-evolved that it approaches \"if you can imagine it: You can render it \n",
    "in LaTeX\". The best advice I can offer: Give yourself plenty of time to fine tune because LaTeX is \n",
    "super precise but super finicky. This last example uses a backslash comma to create a little horizontal space. \n",
    "\n",
    "\n",
    "$m^at^h\\; = \\; \\pi^\\alpha$\n",
    "\n",
    "\n",
    "### Tables\n",
    "\n",
    "\n",
    "Pipes `|`, colons `:` and hyphens `-` can be used to create tables. \n",
    "\n",
    "\n",
    "| Streichduo | PDF | SHO $\\nu$ (khz) | What? | Ok |\n",
    "| :- | -: | :-: | :-: | :-\n",
    "| Bach | Eulerian | 14. | Why?\n",
    "| Mozart | Gaussian | 17.922 | | this column is distended to accommodate a long remark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-cancellation",
   "metadata": {},
   "source": [
    "## Reducing Datasets\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### Summary\n",
    "\n",
    "We are working with extended multi-dimensional oceanography datasets. These are often in \n",
    "NetCDF format and must be sliced, diced, filtered, differentiated and plotted for comparison.\n",
    "There is a lot to do. In order to carry the up-front visual message the very first notebook\n",
    "covers Oregon Slope Base shallow profiler sensors; using a locally available dataset. The\n",
    "reason for this is the data can \"fit\" inside a [GitHub repo](https://github.com/robfatland/ocean)\n",
    "and the notebook can therefore\n",
    "run in binder without any preparation beyond \n",
    "[clicking the binder badge](https://mybinder.org/v2/gh/robfatland/ocean/HEAD)\n",
    "and waiting \n",
    "a couple minutes while the binder (sandbox) version of the repo loads. So in this section\n",
    "let's cover how to do this sort of subsetting. The sensor sampling rate is often once or twice\n",
    "per second so a big part of this is down-sampling to one sample per minute. \n",
    "\n",
    "\n",
    "### Oregon Slope Base shallow profiler sensor abbreviation table \n",
    "\n",
    "\n",
    "- `ctdpf` 3 observables: salinity, temperature and dissolved oxygen against depth\n",
    "- `pCO2`  ...carbonate chemistry: No data at this time\n",
    "- `nutnr` 1 observable: nitrate (used on noon/midnight ascent)\n",
    "- `phsen` 1 observable: pH (used on noon/midnight descent with stops)\n",
    "- `flort` 3 observables: Fluorometer for chlorophyll concentration, cdom fluorescence, backscatter (triplet)\n",
    "- `spkir` 7 observables: spectral irradiance; down-welling\n",
    "- `parad` 1 observable: photosynthetically available radiation, sunlight available for photosynthesis\n",
    "\n",
    "<BR>\n",
    "\n",
    "- need an entry for spectrophotometer; which is treated a little differently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-rabbit",
   "metadata": {},
   "source": [
    "### Prefatory note: 'Super slow' resampling problem\n",
    "\n",
    "\n",
    "* A Dataset operation that should take seconds takes *hours*\n",
    "    * Spectrophotometer has 86 channels: 'One observation with 86 values'\n",
    "    * Associated: Depth and time per each observation; suppose 14k observations per profile \n",
    "    * Result: Multiply-indexed data.\n",
    "    * `time` is the default dimension (superseding `obs`) but we want sort by depth\n",
    "        * Depth bins contain many observations\n",
    "    * Resample 200m at 0.25m bins: 86 x 14000 data values to average into 86 x 800 bins\n",
    "        * Task takes hours\n",
    "\n",
    "The solution comes from page 137 of PDSH, on **Rearranging Multi-Indices**\n",
    "and **Sorted and unsorted indices**; quoting: \n",
    "\n",
    "> ***Rearranging Multi-indices***<BR>\n",
    "One of the keys to working with multiply indexed data is knowing how to effectively transform the data. \n",
    "There are a number of operations that will preserve all the information in the dataset, but rearrange \n",
    "it for the purposes of various computations. [...] There are many [ways] to finely control the rearrangement\n",
    "of data between heirarchical indices and columns.\n",
    "    \n",
    "> ***Sorted and unsorted indices***<BR>\n",
    "Earlier, we briefly mentioned a caveat, but we should emphasize it more here. \n",
    "*Many of the `MultiIndex`slicing operations will fail if the index is not sorted.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-retro",
   "metadata": {},
   "source": [
    "### One minute resampling\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "We want the repository to contain a certain amount of downsampled data. The time window\n",
    "is January 2019 to start with. The sample rate is one sample per minute. The ascent rate \n",
    "is about 180 meters in about 45 minutes or roughly 4 meters per minute. This is adequate to see basic\n",
    "water column structure through the sensor 'lens'. \n",
    "\n",
    "\n",
    "* 86400 seconds per day, 1440 minutes per day, about 45k samples per January at 1Min intervals\n",
    "\n",
    "We have two mechanisms available in `xarray Datasets`: `.sel(time=slice(dt64-A, dt64-B))`\n",
    "and `ds_1min = ds.resample(time='1Min').mean()`. Also we can use `.std()` if we wish to\n",
    "expand into standard deviation signals. \n",
    "\n",
    "\n",
    "There is a problem, however: The `xarray .resample()` method can hang. This may be a time-sort\n",
    "issue where the prior `.sel()` time slice is not understood as a monotonic time dimension.\n",
    "Whatever the cause there is a work-around using `pandas Dataframes`. Start by loading the \n",
    "full-resolution data from a data drive: \n",
    "\n",
    "\n",
    "```\n",
    "ctd_source = '/data/rca/ctd/'\n",
    "ctd_data = 'osb_sp_ctdpf_2019.nc'\n",
    "ds_CTD = xr.open_dataset(ctd_source + ctd_data)\n",
    "ds_CTD\n",
    "\n",
    "time_jan1 = dt64('2019-01-01')\n",
    "time_feb1 = dt64('2019-02-01')\n",
    "ds_CTD_jan2019 = ds_CTD.sel(time=slice(time_jan1, time_feb1))\n",
    "ds_CTD_jan2019\n",
    "```\n",
    "\n",
    "This .resample() code hangs:\n",
    "\n",
    "\n",
    "```\n",
    "# forever fail\n",
    "# ds_CTD_jan2019_1Min = ds_CTD_time_slice.sel(time=slice(time_jan1, \\\n",
    "#                                                        time_feb1)).resample(time='1Min').mean()\n",
    "# ds_CTD_jan2019_1Min\n",
    "```\n",
    "\n",
    "Here is the pandas dataframe workaround:\n",
    "\n",
    "\n",
    "```\n",
    "df   = ds_CTD_jan2019.to_dataframe().resample(\"1Min\").mean()\n",
    "vals = [xr.DataArray(data=df[c], dims=['time'], coords={'time':df.index}, attrs=ds_CTD_jan2019[c].attrs) for c in df.columns]\n",
    "ds_CTD_jan2019_1Min = xr.Dataset(dict(zip(df.columns, vals)), attrs=ds_CTD_jan2019.attrs)\n",
    "ds_CTD_jan2019_1Min\n",
    "```\n",
    "\n",
    "\n",
    "This writes out the down-sampled data as a new NetCDF file. Notice it is a \"local\" data directory.\n",
    "\n",
    "\n",
    "```\n",
    "osb_ctd_nc_file = \"./data/rca/ctd/osb_ctd_jan2019_1min.nc\"\n",
    "ds_CTD_jan2019_1Min.to_netcdf(osb_ctd_nc_file)\n",
    "```\n",
    "\n",
    "This loads the resulting data; so this is the starting point of the data exploration code: \n",
    "\n",
    "\n",
    "```\n",
    "ds_CTD = xr.open_dataset(osb_ctd_nc_file)\n",
    "ds_CTD\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-boating",
   "metadata": {},
   "source": [
    "Flourometry code redux: For OSB shallow profiler triplet, to 1Min samples, JAN 2019\n",
    "\n",
    "\n",
    "```\n",
    "ds_Fluorometer = xr.open_dataset('/data/rca/fluorescence/osb_sp_flort_2019.nc')\n",
    "time_jan1, time_feb1 = dt64('2019-01-01'), dt64('2019-02-01')\n",
    "ds_Fluor_jan2019 = ds_Fluorometer.sel(time=slice(time_jan1, time_feb1))\n",
    "df               = ds_Fluor_jan2019.to_dataframe().resample(\"1Min\").mean()\n",
    "vals             = [xr.DataArray(data=df[c], dims=['time'], coords={'time':df.index}, \\\n",
    "                    attrs=ds_Fluor_jan2019[c].attrs) for c in df.columns]\n",
    "xr.Dataset(dict(zip(df.columns, vals)), \\\n",
    "           attrs=ds_Fluor_jan2019.attrs).to_netcdf('./data/rca/fluorescence/osb_sp_fluor_jan2019.nc')\n",
    "```\n",
    "\n",
    "Spectral irradiance stopgap version: Break out by spectrum (should be dimension of just one file).\n",
    "\n",
    "```\n",
    "spectral_irradiance_source = '/data/rca/irradiance/'\n",
    "spectral_irradiance_data = 'osb_sp_spkir_2019.nc'\n",
    "ds_spectral_irradiance = xr.open_dataset(spectral_irradiance_source + spectral_irradiance_data)\n",
    "ds_spectral_irradiance\n",
    "time_jan1, time_feb1 = dt64('2019-01-01'), dt64('2019-02-01')\n",
    "ds_Irr_jan2019 = ds_spectral_irradiance.sel(time=slice(time_jan1, time_feb1))\n",
    "df = [ds_Irr_jan2019.sel(spectra=s).to_dataframe().resample(\"1Min\").mean() for s in ds_Irr_jan2019.spectra]\n",
    "r = [xr.Dataset(dict(zip(q.columns, \n",
    "                         [xr.DataArray(data=q[c], dims=['time'], coords={'time':q.index}, \\\n",
    "                                       attrs=ds_Irr_jan2019[c].attrs) for c in q.columns] \\\n",
    "                    )   ), \n",
    "                attrs=ds_Irr_jan2019.attrs)\n",
    "    for q in df]\n",
    "for i in range(7): r[i].to_netcdf('./data/rca/irradiance/osb_sp_irr_spec' + str(i) + '.nc')\n",
    "```\n",
    "\n",
    "\n",
    "Spectral irradiance related skeleton code showing use of `.isel(spectra=3)`: \n",
    "\n",
    "\n",
    "```\n",
    "ds = ds_spkir.sel(time=slice(time0, time1))\n",
    "da_depth = ds.int_ctd_pressure.resample(time='1Min').mean()\n",
    "dsbar = ds.resample(time='1Min').mean()\n",
    "dsstd = ds.resample(time='1Min').std()\n",
    "dsbar.spkir_downwelling_vector.isel(spectra=3).plot()\n",
    "\n",
    "\n",
    "plot_base_dimension = 4\n",
    "indices = [0, 1, 2, 3, 4, 5, 6]\n",
    "n_indices = len(indices)\n",
    "da_si, da_st = [], []\n",
    "\n",
    "\n",
    "for idx in indices: \n",
    "    da_si.append(dsbar.spkir_downwelling_vector.isel(spectra=idx))\n",
    "    da_st.append(dsstd.spkir_downwelling_vector.isel(spectra=idx))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(n_indices, 2, figsize=(4*plot_base_dimension, plot_base_dimension*n_indices), /\n",
    "           sharey=True, tight_layout=True)\n",
    "\n",
    "\n",
    "axs[0][0].scatter(da_si[0], da_depth, marker=',', s=1., color='k') \n",
    "axs[0][0].set(ylim = (200., 0.), xlim = (-.03, .03), title='spectral irradiance averaged')\n",
    "axs[0][1].scatter(da_st[0], da_depth, marker=',', s=1., color='r')\n",
    "axs[0][1].set(ylim = (200., 0.), xlim = (0., .002), title='standard deviation')\n",
    "\n",
    "\n",
    "for i in range(1, n_indices):\n",
    "    axs[i][0].scatter(da_si[i], da_depth, marker=',', s=1., color='k')\n",
    "    axs[i][0].set(ylim = (200., 0.), xlim = (-.03, .03))\n",
    "    axs[i][1].scatter(da_st[i], da_depth, marker=',', s=1., color='r')\n",
    "    axs[i][1].set(ylim = (200., 0.), xlim = (0., .002))\n",
    "```\n",
    "\n",
    "Code for PAR\n",
    "\n",
    "```\n",
    "par_source = '/data/rca/par/'\n",
    "par_data = 'osb_sp_parad_2019.nc'\n",
    "ds_par = xr.open_dataset(par_source + par_data)\n",
    "time_jan1 = dt64('2019-01-01')\n",
    "time_feb1 = dt64('2019-02-01')\n",
    "ds_par_jan2019 = ds_par.sel(time=slice(time_jan1, time_feb1))\n",
    "df   = ds_par_jan2019.to_dataframe().resample(\"1Min\").mean()\n",
    "vals = [xr.DataArray(data=df[c], dims=['time'], coords={'time':df.index}, attrs=ds_par_jan2019[c].attrs) for c in df.columns]\n",
    "ds_par_jan2019_1Min = xr.Dataset(dict(zip(df.columns, vals)), attrs=ds_par_jan2019.attrs)\n",
    "osb_par_nc_file = \"./data/rca/par/osb_sp_par_jan2019.nc\"\n",
    "ds_par_jan2019_1Min.to_netcdf(osb_par_nc_file)\n",
    "```\n",
    "\n",
    "PAR view: during shallow profiler rise/fall sequences\n",
    "\n",
    "```\n",
    "t0, t1 = '2019-07-17T13', '2019-07-18T05'\n",
    "t0, t1 = '2019-07-17T18:40', '2019-07-17T19:40'\n",
    "t0, t1 = '2019-07-17T21', '2019-07-17T23:00'        # These are the nitrate profiles\n",
    "t0, t1 = '2019-07-18T21', '2019-07-18T23:00'\n",
    "t0, t1 = '2019-07-19T21', '2019-07-19T23:00'\n",
    "t0, t1 = '2019-07-17T18:40', '2019-07-17T19:40'     # These are the profiles prior to nitrate\n",
    "t0, t1 = '2019-07-18T18:40', '2019-07-18T19:40'\n",
    "t0, t1 = '2019-07-19T18:40', '2019-07-19T19:40'\n",
    "da = ds_parad.sel(time=slice(t0, t1)).par_counts_output\n",
    "p=da.plot.line(marker='o', figsize = (14,8), markersize=1, yincrease = True)\n",
    "```\n",
    "\n",
    "Staged 'nitrate' profile compared with 'normal' profile\n",
    "\n",
    "```\n",
    "t0, t1 = '2019-07-19T20:30', '2019-07-19T23:50'               # USE THIS!! This is a good nitrate profile time bracket\n",
    "t0, t1 = '2019-07-19T18:40', '2019-07-19T19:40'\n",
    "da = ds_parad.sel(time=slice(t0, t1)).int_ctd_pressure\n",
    "p=da.plot.line(marker='o', figsize = (14,8), markersize=1, yincrease = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-moses",
   "metadata": {},
   "source": [
    "## Plotting \n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### Summary\n",
    "\n",
    "Here I use [**`pyplot`**](https://plotly.com/python/), the Python graphing library usually imported as `plt`. \n",
    "Plotly is part of the matplotlib package. It is super confusing unless one has\n",
    "an extensive block of time to study it with care. In particular\n",
    "we need to be experts at \"drawing charts\" both with `.scatter` and `.plot`. Furthermore\n",
    "there is a quick-and-dirty `.plot` within XArray Datasets that can save time during\n",
    "development by giving a quick sanity check. In this section I will cover the basics \n",
    "of charting data in this context.\n",
    "\n",
    "### To investigate\n",
    "\n",
    "* Jake talks about Seaborn in PDSH; worth a look\n",
    "\n",
    "### Diving in\n",
    "\n",
    "### Sharing an axis\n",
    "\n",
    "\n",
    "Suppose a figure has an axis or a list of axes. These have a method `.twiny()` which creates\n",
    "a copy that can have its own x-axis stipulated. Same thing for `.twinx()`. This is demonstrated\n",
    "in the **Ocean 01 D Photic Zone Reduction** notebook. It risks cluttering the chart with the idea\n",
    "of condensing information.\n",
    "\n",
    "### Grid of charts\n",
    "\n",
    "This is example code for time-series data. It sets up a 3 x 3 grid of charts. These are matched to a 2-D set of\n",
    "axes (the 'a' variable) with both the scatter() and plot() constructs.\n",
    "\n",
    "```\n",
    "rn = range(9); rsi = range(7)\n",
    "\n",
    "p,a=plt.subplots(3, 3, figsize=(14,14))    # first 3 is vertical count, second 3 is horizontal count\n",
    "\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "a[0,0].plot(ctdF.time, ctdF.depth, color='r');                                  a[0,0].set(ylim=(200.,0.), title='Depth')\n",
    "a[0,1].plot(ctdF.time, ctdF.salinity, color='k');                               a[0,1].set(title='Salinity')\n",
    "a[0,2].plot(ctdF.time, ctdF.temperature, color='b');                            a[0,2].set(title='Temperature')\n",
    "a[1,0].plot(ctdF.time, ctdF.dissolved_oxygen, color='b');                       a[1,0].set(title='Dissolved Oxygen')\n",
    "a[1,1].scatter(phF.time.values, phF.ph_seawater.values, color='r');             a[1,1].set(title='pH')\n",
    "a[1,2].scatter(nitrateF.time.values, nitrateF.scn.values, color='k');           a[1,2].set(title='Nitrate')\n",
    "a[2,0].plot(parF.time, parF.par_counts_output, color='k');                      a[2,0].set(title='Photosynthetic Light')\n",
    "a[2,1].plot(fluorF.time, fluorF.fluorometric_chlorophyll_a, color='b');         a[2,1].set(title='Chlorophyll')\n",
    "a[2,2].plot(siF.time, siF.si0, color='r');                                      a[2,2].set(title='Spectral Irradiance')\n",
    "\n",
    "a[2,0].text(dt64('2017-08-21T07:30'), 155., 'local midnight', rotation=90, fontsize=15, color='blue', fontweight='bold')\n",
    "a[2,2].text(dt64('2017-08-21T07:30'), 4.25, 'local midnight', rotation=90, fontsize=15, color='blue', fontweight='bold')\n",
    "\n",
    "tFmt   = mdates.DateFormatter(\"%H\")                 # an extended format for strftime() is \"%d/%m/%y %H:%M\"\n",
    "t0, t1 = ctdF.time[0].values, ctdF.time[-1].values  # establish same time range for each chart\n",
    "tticks = [dt64('2017-08-21T06:00'), dt64('2017-08-21T12:00'), dt64('2017-08-21T18:00')]\n",
    "\n",
    "for i in rn: j, k = i//3, i%3; a[j, k].set(xlim=(t0, t1),xticks=tticks); a[j, k].xaxis.set_major_formatter(tFmt)\n",
    "print('')\n",
    "```\n",
    "\n",
    "\n",
    "Please note that Software Carpentry (Python) uses a post-facto approach to axes. \n",
    "In what follows there is implicit use of numpy 'collapse data along a particular\n",
    "dimension' using the `axis` keyword. So this is non-trivial code; but main point \n",
    "it shows adding axes to the figure.\n",
    "\n",
    "```\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "\n",
    "axes1 = fig.add_subplot(1,3,1)\n",
    "axes2 = fig.add_subplot(1,3,2)\n",
    "axes3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "avg_data = numpy.mean(data, axis=0)\n",
    "min_data = numpy.min(data, axis=0)\n",
    "max_data = numpy.max(data, axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-contributor",
   "metadata": {},
   "source": [
    "### Making animations\n",
    "\n",
    "[Top](#Introduction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bca32a7",
   "metadata": {},
   "source": [
    "This section was lifted from the BioOptics.ipynb notebook and simplified. It illustrates **overloading** a chart to \n",
    "show multiple sensor profiles evolving over time (frames). It also illustrates using markers along a line plot to\n",
    "emphasize observation spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "qualified-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This code creates the animation; requires some time so it is commented out for now.\n",
    "# anim = animation.FuncAnimation(fig, AnimateChart, init_func=AnimateInit, \\\n",
    "#                                frames=nframes, interval=250, blit=True, repeat=False)\n",
    "#\n",
    "# Use 'HTML(anim.to_html5_video())'' for direct playback\n",
    "# anim.save(this_dir + '/Images/animations/multisensor_animation.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230db14",
   "metadata": {},
   "source": [
    "## Binder-friendly playback\n",
    "\n",
    "\n",
    "The cell above creates an animation file that is stored within this repository. \n",
    "The cell below plays it back (for example in **binder**) to show multiple profile animations.\n",
    "Nitrate is intermittent, appearing as a sky-blue line in 2 of every 9\n",
    "frames. The remaining sensors are present in each frame.\n",
    "\n",
    "\n",
    "There animation begins March 1 2021 and proceeds at a rate of nine frames (profiles) per day.\n",
    "Change playback speed using the video settings control at lower right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binder-friendly playback\n",
    "from IPython.display import HTML, Video\n",
    "import os\n",
    "Video(os.getcwd() + '../Images/animations/multisensor_animation.mp4', embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "soviet-parker",
   "metadata": {},
   "source": [
    "## Multimedia\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "<BR>\n",
    "<img src=\".././Images/fauna/dubious.png\" style=\"float: left;\" alt=\"dubious person trying to eat kelp\" width=\"100\"/>\n",
    "<div style=\"clear: left\">\n",
    "<BR>\n",
    "\n",
    "This png file of a child dubiously eating kelp illustrates transparent pixels. \n",
    "There are lots of free/native apps to do this. \n",
    "    \n",
    "    \n",
    "### Summary\n",
    "\n",
    "We want to include (in order of importance) images, animations of data and sound clips. The above image,\n",
    "incidentally, includes transparent pixels: Done on a PC using Paint 3D, where the process is a bit convoluted. \n",
    "\n",
    "### Images\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "The cleanest presentation for static images in my experience so far is to use HTML in a \n",
    "markdown cell. It looks as follows where `<BR>` is a line break giving some spacing. Note\n",
    "the relative path.\n",
    "\n",
    "```\n",
    "<BR>\n",
    "<img src=\"./../Images/vessels/revelle.jpg\" style=\"float: left;\" alt=\"ship and iceberg photo\" width=\"900\"/>\n",
    "<div style=\"clear: left\">\n",
    "<BR>\n",
    "```\n",
    "\n",
    "It is possible to include images in Python cells using PIL but it was more of a chore.\n",
    "\n",
    "\n",
    "### Animations\n",
    "    \n",
    "    \n",
    "[Top](#Introduction)\n",
    "\n",
    "    \n",
    "Once an mp4 file is written to the file system playback is simple: Import `Video` from `IPython.display`\n",
    "and play the file back using `Video` setting the `embed` flag True.\n",
    "\n",
    "\n",
    "```\n",
    "from IPython.display import Video\n",
    "Video(\"./<some_animation>.mp4\", embed=True)\n",
    "```\n",
    "\n",
    "Alternative to `, embed=True`: Turn on the inline back-end using `%matplotlib inline` line magic\n",
    "\n",
    "    \n",
    "#### YouTube video playback\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "    \n",
    "```\n",
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('sjfsUzECqK0')\n",
    "```\n",
    "    \n",
    "### Sound clips\n",
    "\n",
    "    \n",
    "[Top](#Introduction)\n",
    "\n",
    "```\n",
    "from IPython.display import Audio\n",
    "Audio(\"<audiofile>.mp3\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-spectrum",
   "metadata": {},
   "source": [
    "## XArray Datasets and DataArrays\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### Summary\n",
    "\n",
    "There are a million little details about working with XArray Datasets, DataArrays, numpy arrays, pandas DataFrames,\n",
    "pandas arrays... let's begin! The main idea is that a **DataArray** is an object containing, in the spirit of \n",
    "the game, one sort of data; and a **Dataset** is a collection of associated **DataArray**s. \n",
    "\n",
    "\n",
    "### XArray ***Dataset*** basics\n",
    "\n",
    "**Datasets** abbreviated `ds` have four components { dimensions, coordinates, data variables, \n",
    "attributes }.\n",
    "\n",
    "\n",
    "The matter of a name in relation to a **DataArray** is worth describing so to do. \n",
    "\n",
    "\n",
    "```\n",
    "ds.variables                                  # to do detail on what this is in relation to 'four'\n",
    "\n",
    "ds.data_vars                                  # to do elaborate 'dict-like object'\n",
    "for dv in ds.data_vars: print(dv)\n",
    "    \n",
    "choice = 2\n",
    "this_data_var = list(ds.data_vars)[choice]\n",
    "print(this_data_var)\n",
    "\n",
    "ds.coords\n",
    "ds.dims\n",
    "ds.attrs\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Load via `open_mfdataset()` with dimension swap from `obs` to `time`\n",
    "\n",
    "\n",
    "A single NetCDF (`.nc`) file can be loaded directly into an XArray Dataset using `xr.open_dataset(fnm)`. \n",
    "Multiple files -- provided they are or can be made to be mutually agreeable -- can be opened as a single\n",
    "XArray Dataset via `xr.open_mfdataset(fnm*.nc)`. Noting two things: `mf` stands for `multi-file`; and \n",
    "the filename includes a wildcard `fnm*` so that it will cover all the files needed. \n",
    "\n",
    "\n",
    "The asterisk (e.g. 'nutnr_a_sample*') is wildcard construction to treat multiple files as one Dataset. \n",
    "The key feature here is that ds is now monolithic, not fragmented like the source files. \n",
    "\n",
    "```\n",
    "strRoot = 'rca/2019/depl*'\n",
    "strSite = 'SBPS*'\n",
    "strPlatform = 'SF*'\n",
    "\n",
    "def lass_preprocessor(fds): return fds.swap_dims({'obs':'time'})\n",
    "ds = xr.open_mfdataset(data_dir + strRoot + strSite + strPlatform + 'nutnr_a_sample*.nc', \n",
    "                       preprocess = lass_preprocessor, \n",
    "                       concat_dim='time', combine='by_coords')\n",
    "ds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elder-compound",
   "metadata": {},
   "source": [
    "#### Obstacle: Getting information out of a Dataset\n",
    "\n",
    "There is a sort of comprehension / approach that I have found hard to internalize.\n",
    "With numpy ndarrays, XArray Datasets, etcetera there is this \"how do I get at it?\"\n",
    "problem. As this documentation evolves I will try and articulate the most helpful\n",
    "mindset. The starting point is that Datasets are built as collections of DataArrays; \n",
    "and these have an indexing protocol the merges with a method protocol (`sel`, `merge`\n",
    "and so on) where the end-result code that does what I want is inevitably very \n",
    "elegant. So it is a process of learning that elegant sub-language...\n",
    "\n",
    "\n",
    "#### Example: Get a time dimension value as `datetime64` from a Dataset using an index\n",
    "\n",
    "Supposing the linked coordinate / dimension is `time`. This can be referenced using \n",
    "`dt.time[i]` but it will come out as a 1-D 1-element DataArray. Therefore it is \n",
    "necessary to do more; as in this example where we select the `.data` field and cast\n",
    "the resulting ndarray (with one element) as a `dt64`. This works but feels \n",
    "incomplete; maybe unnecessarily clunky.\n",
    "\n",
    "```dt64(ds.time[i].data)```\n",
    "\n",
    "\n",
    "#### Example: XArray transformation flow\n",
    "    \n",
    "    \n",
    "As an example of the challenge of learning `XArray`: The reduction of this data to binned profiles\n",
    "requires a non-trivial workflow. A naive approach can result in a calculation that should take \n",
    "a seconds run for hours. (A key idea of this workflow -- the sortby() step -- is found on page 137 of **PDSH**.)\n",
    "    \n",
    "    \n",
    "- `swap_dims()` to substitute `pressure` for `time` as the ordinate dimension\n",
    "- `sortby()` to make the `pressure` dimension monotonic\n",
    "- Create a pressure-bin array to guide the subsequent data reduction\n",
    "- `groupby_bins()` together with `mean()` to reduce the data to a 0.25 meter quantized profile\n",
    "- use `transpose()` to re-order wavelength and pressure, making the resulting `DataArray` simpler to plot\n",
    "- accumulate these results by day as a list of `DataArrays`\n",
    "- From this list create an `XArray Dataset`\n",
    "- Write this to a new NetCDF file\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Example: XArray Dataset subset and chart\n",
    "\n",
    "Let's suppose we open a NetCDF temperature file as a Dataset `ds` where `time` is the dimension. We want to \n",
    "focus on a time range defined by two numpy `datetime64` values `t0` and `t1`. Let's extract a smaller\n",
    "Dataset that only has data from that time range; and let's keep the depth value `z` associated with \n",
    "each measurement. In this way we can chart temperature against either time or depth. \n",
    "\n",
    "```\n",
    "ds = xr.open_dataset(\"some file\")\n",
    "# t0 and t1 taken as established\n",
    "ds = ds.sel(time=slice(t0, t1))\n",
    "ds\n",
    "```\n",
    "\n",
    "Now we have `ds` as a Dataset with temperature and depth. Unfortunately the temperature Data Variable\n",
    "has a stupid name inherited from the engineering of the system, namely \n",
    "`sea_water_temperature_profiler_depth_enabled`. So let's rename it more sensibly. We do this\n",
    "using the `rename()` method which is given a dictionary. Notice the clobber-assignment `ds = ds.etcetera`.\n",
    "\n",
    "```\n",
    "ds = ds.rename({'sea_water_temperature_profiler_depth_enabled':'temperature'})\n",
    "```\n",
    "\n",
    "Now we can plot this against the default dimension `time` using `.plot`: \n",
    "\n",
    "```\n",
    "ds.temperature.plot()\n",
    "```\n",
    "\n",
    "However this is not a chart of temperature against depth. Really the single profiler ascent chart\n",
    "should have depth on the y-axis and temperature on the x-axis. This is probably possible using the \n",
    "above `.plot()` method but this is how to do it in matplotlib as a single chart.\n",
    "\n",
    "\n",
    "```\n",
    "fig, axs = plt.subplots(figsize=(12,4), tight_layout=True)\n",
    "axs.plot(ds.temperature, -ds.z, marker='.', markersize=9., color='k', markerfacecolor='r')\n",
    "axs.set(ylim = (200., 0.), title='temperature against depth')\n",
    "```\n",
    "\n",
    "Here please note that depth `z` was natively \"positive is up from sea surface\", i.e. the data \n",
    "are negative-valued; so I used `-ds.z` and then stipulated the axis range as `200., 0.`. \n",
    "\n",
    "\n",
    "This procedure gets more involved when building a multi-chart view and again when sharing an \n",
    "axis to superimpose data from two different sensor types. \n",
    "\n",
    "\n",
    "#### More cleanup of Datasets: rename() and drop()\n",
    "\n",
    "* Use `ds.rename(dictionary-of-from-to)` to rename data variables in a Dataset\n",
    "* Use `ds.drop(string-name-of-data-var)` to get rid of a data variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-blackjack",
   "metadata": {},
   "source": [
    "### XArray ***DataArray*** name and length\n",
    "\n",
    "\n",
    "```\n",
    "sensor_t.name\n",
    "\n",
    "len(sensor_t)\n",
    "len(sensor_t.time)           # gives same result\n",
    "```\n",
    "\n",
    "What is the name of the controlling dimension?\n",
    "\n",
    "```\n",
    "if sensor_t.dims[0] == 'time': print('time is dimension zero')\n",
    "```\n",
    "\n",
    "Equivalent; but the second version permits reference by \"discoverable\" string.\n",
    "\n",
    "\n",
    "```\n",
    "sensor_t = ds_CTD_time_slice.seawater_temperature\n",
    "sensor_t = ds_CTD_time_slice['seawater_temperature']\n",
    "```\n",
    "\n",
    "#### Plotting with scaling and offsetting\n",
    "\n",
    "Suppose I wish to shift some data left to contrast it with some other data (where they would clobber one another)...\n",
    "\n",
    "```\n",
    "sensor_t + 0.4\n",
    "```\n",
    "\n",
    "Suppose I wish to scale some data in a chart to make it easier to interpret given a fixed axis range\n",
    "\n",
    "```\n",
    "sensor_t * 10.               # this fails by trying to make ten copies of the array\n",
    "\n",
    "np.ones(71)*3.*smooth_t      # this works by creating an inner product\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-series",
   "metadata": {},
   "source": [
    "## Pandas Series and DataFrames\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### Summary\n",
    "\n",
    "### DataFrames\n",
    "\n",
    "DataFrames:\n",
    "\n",
    "* constructor takes `data=<ndarray>` and both `index` and `columns` arguments... \n",
    "    * ...2 dimensions only: higher dimensions and they say 'use XArray'\n",
    "    * ...and switching required a `.T` transpose\n",
    "* indexing by column and row header values, separated as in `[column_header][row_header]`\n",
    "    * as this reverses order from ndarrays: Better confirm... seems to be the case\n",
    "    * skip index/columns: defaults to integers.\n",
    " \n",
    "Below this section we go into n-dimensional arrays in Numpy, the *ndarray*. Here we take this \n",
    "for granted and look at the relationship with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mediterranean-schedule",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ndarray from a list of lists (notice no comma delimiter):\n",
      "\n",
      " [['l' 'i' 's' 't' '1']\n",
      " ['s' 'c' 'n' 'd' '2']\n",
      " ['t' 'h' 'r' 'd' '3']] \n",
      "\n",
      "and indexing comparison: first [0][2] then [2][0]: s t\n",
      "\n",
      "and tuplesque indexing [0, 2] or [2, 0] equivalently gives: s t\n",
      "\n",
      "So ndarrays index [slow][fast] equivalent to [row][column]\n",
      "\n",
      "\n",
      "Moving on to DataFrames:\n",
      "\n",
      "\n",
      "     col_a col_b col_c col_d col_e\n",
      "2row     l     i     s     t     1\n",
      "4row     s     c     n     d     2\n",
      "6row     t     h     r     d     3 \n",
      "\n",
      "is a DataFrame from the ndarray; so now index [\"col_c\"][\"6row\"]: r\n",
      "\n",
      "Here is a Dataframe from a transpose of the ndarray\n",
      "\n",
      "       2row 4row 6row\n",
      "col_a    l    s    t\n",
      "col_b    i    c    h\n",
      "col_c    s    n    r\n",
      "col_d    t    d    d\n",
      "col_e    1    2    3 \n",
      "\n",
      "indexing 2row then col_e: 1\n",
      "\n",
      "So the column of a DataFrame is indexed first, then the row: Reverses the sense of the 2D ndarray.\n",
      "\n",
      "Now skipping the \"index=\"\" argument so the row labels default to integers:\n",
      "\n",
      "  col_a col_b col_c col_d col_e\n",
      "0     l     i     s     t     1\n",
      "1     s     c     n     d     2\n",
      "2     t     h     r     d     3 \n",
      "\n",
      "...so now indexing [\"col_d\"][0]: t \n",
      "\n",
      "      0  1  2  3  4\n",
      "2row  l  i  s  t  1\n",
      "4row  s  c  n  d  2\n",
      "6row  t  h  r  d  3 \n",
      "\n",
      "having done it the other way: used index= but not columns=. Here is element [0][\"4row\"]: s\n",
      "\n",
      "\n",
      "Starting from an XArray Dataset and using .to_dataframe() we arrive at a 2D structure.\n",
      "\n",
      "For example: df = ds_CTD.seawater_pressure.to_dataframe()\n",
      " \n",
      "The problem is that the resulting dataframe may not be indexed (row sense) using integers. A fix\n",
      "is necessary to override the index and columns attributes of the dataframe, as in:\n",
      " \n",
      "             df.index=range(len(df))\n",
      "             df.columns=range(1)\n",
      " \n",
      "results in a dataframe that one can index with integers [0] for column first then [n] for row.\n",
      "This example came from the profile time series analysis to get ascent start times and so on.\n",
      "The problem is it is a case of too much machinery. It is far simpler to use a pandas Series.\n"
     ]
    }
   ],
   "source": [
    "###################\n",
    "#\n",
    "# A micro study of ndarray to DataFrame translation\n",
    "#\n",
    "###################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Here is an ndarray construction from a built list of lists (not used in what follows): \n",
    "# arr = np.array([range(i, i+5) for i in [2, 4, 6]])                                       \n",
    "#     ... where the range() runs across columns; 2 4 6 are rows\n",
    "\n",
    "# ndarray construction: Notice all list elements are of the same type (strings)\n",
    "arr = np.array([['l','i','s','t','1'],['s','c','n','d','2'],['t','h','r','d', '3']])\n",
    "\n",
    "print('\\nndarray from a list of lists (notice no comma delimiter):\\n\\n', arr, \\\n",
    "      '\\n\\nand indexing comparison: first [0][2] then [2][0]:', arr[0][2], arr[2][0]) \n",
    "print('\\nand tuplesque indexing [0, 2] or [2, 0] equivalently gives:', arr[0,2], arr[2,0])\n",
    "print('\\nSo ndarrays index [slow][fast] equivalent to [row][column]\\n\\n\\nMoving on to DataFrames:\\n\\n')\n",
    "\n",
    "rowlist=[\"2row\", \"4row\", \"6row\"]\n",
    "columnlist = [\"col_a\", \"col_b\", \"col_c\", \"col_d\", \"col_e\"]\n",
    "df = pd.DataFrame(data=arr, index=rowlist, columns=columnlist)\n",
    "\n",
    "print(df, '\\n\\nis a DataFrame from the ndarray; so now index [\"col_c\"][\"6row\"]:', df['col_c']['6row'])\n",
    "\n",
    "df = pd.DataFrame(data=arr.T, index=columnlist, columns=rowlist)\n",
    "\n",
    "print('\\nHere is a Dataframe from a transpose of the ndarray\\n\\n', df, \\\n",
    "      '\\n\\nindexing 2row then col_e:', df['2row']['col_e'])\n",
    "print('\\nSo the column of a DataFrame is indexed first, then the row: Reverses the sense of the 2D ndarray.\\n')\n",
    "print('Now skipping the \"index=\"\" argument so the row labels default to integers:\\n')\n",
    "\n",
    "df = pd.DataFrame(data=arr, columns=columnlist)\n",
    "\n",
    "print(df, '\\n\\n...so now indexing [\"col_d\"][0]:', df['col_d'][0], '\\n')\n",
    "\n",
    "df = pd.DataFrame(data=arr, index=rowlist)\n",
    "\n",
    "print(df, '\\n\\nhaving done it the other way: used index= but not columns=. Here is element [0][\"4row\"]:', \\\n",
    "      df[0]['4row'])\n",
    "\n",
    "\n",
    "print('\\n\\nStarting from an XArray Dataset and using .to_dataframe() we arrive at a 2D structure.\\n')\n",
    "print('For example: df = ds_CTD.seawater_pressure.to_dataframe()')\n",
    "print(' ')\n",
    "print('The problem is that the resulting dataframe may not be indexed (row sense) using integers. A fix')\n",
    "print('is necessary to override the index and columns attributes of the dataframe, as in:')\n",
    "print(' ')\n",
    "print('             df.index=range(len(df))')\n",
    "print('             df.columns=range(1)')\n",
    "print(' ')\n",
    "print('results in a dataframe that one can index with integers [0] for column first then [n] for row.')\n",
    "print('This example came from the profile time series analysis to get ascent start times and so on.')\n",
    "print('The problem is it is a case of too much machinery. It is far simpler to use a pandas Series.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d6a6de",
   "metadata": {},
   "source": [
    "### Selecting based on a range\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "Suppose we have a DataFrame with a column of timestamps over a broad time range and we would like to focus on only a subset. \n",
    "One approach would be to generate a smaller dataframe that meets the small time criterion and iterate over that.\n",
    "\n",
    "The following cell builds a pandas DataFrame with a date column; then creates a subset DataFrame where only rows in\n",
    "a time range are preserved. This is done twice: First using conditional logic and then using the same with '.loc'. \n",
    "('loc' and 'iloc' are location-based indexing, the first relying on labels and the second on integer position.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a58cc95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[numpy.datetime64('2020-10-11') 7 13 6]\n",
      " [numpy.datetime64('2020-10-12') 7 9 6]\n",
      " [numpy.datetime64('2020-10-13') 7 8 6]\n",
      " [numpy.datetime64('2020-10-14') 7 5 6]\n",
      " [numpy.datetime64('2020-10-15') 7 11 6]]\n",
      "\n",
      "arr[0][2] then [2][0]: 13 2020-10-13\n",
      "\n",
      "and tuplesque indexing [0, 2] or [2, 0] equivalently gives: 13 2020-10-13\n",
      "\n",
      "using conditionals:\n",
      "\n",
      "            date data1 data2 data3\n",
      "day3 2020-10-13     7     8     6\n",
      "day4 2020-10-14     7     5     6 \n",
      "\n",
      "\n",
      "using loc:\n",
      "\n",
      "            date data1 data2 data3\n",
      "day3 2020-10-13     7     8     6\n",
      "day4 2020-10-14     7     5     6\n",
      "\n",
      "notice the results are identical; so it is an open question \"Why use `loc`?\"\n"
     ]
    }
   ],
   "source": [
    "from numpy import datetime64 as dt64, timedelta64 as td64\n",
    "\n",
    "t0=dt64('2020-10-11')\n",
    "t1=dt64('2020-10-12')\n",
    "t2=dt64('2020-10-13')\n",
    "t3=dt64('2020-10-14')\n",
    "t4=dt64('2020-10-15')\n",
    "\n",
    "r0 = dt64('2020-10-12')\n",
    "r1 = dt64('2020-10-15')\n",
    "\n",
    "arr = np.array([[t0,7,13,6],[t1,7,9,6],[t2,7,8,6],[t3,7,5,6],[t4,7,11,6]])\n",
    "\n",
    "print(arr)\n",
    "print('\\narr[0][2] then [2][0]:', arr[0][2], arr[2][0]) \n",
    "print('\\nand tuplesque indexing [0, 2] or [2, 0] equivalently gives:', arr[0,2], arr[2,0])\n",
    "\n",
    "rowlist    = [\"day1\", \"day2\",\"day3\",\"day4\",\"day5\"]\n",
    "columnlist = [\"date\", \"data1\", \"data2\", \"data3\"]\n",
    "df = pd.DataFrame(data=arr, index=rowlist, columns=columnlist)\n",
    "\n",
    "\n",
    "df_conditional = df[(df['date'] > r0) & (df['date'] < r1)]\n",
    "print('\\nusing conditionals:\\n\\n', df_conditional, '\\n')\n",
    "\n",
    "\n",
    "df_loc = df.loc[(df['date'] > r0) & (df['date'] < r1)]\n",
    "print('\\nusing loc:\\n\\n', df_loc)\n",
    "\n",
    "print('\\nnotice the results are identical; so it is an open question \"Why use `loc`?\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f3689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "celtic-following",
   "metadata": {},
   "source": [
    "## Numpy ndarrays\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "### Summary\n",
    "\n",
    "### Diving in \n",
    "\n",
    "numpy ndarrays \n",
    "\n",
    "* do not have row and column headers; whereas pandas DataFrames do have typed headers\n",
    "* indexing has an equivalence of `[2][0]` to `[2,0]` \n",
    "    * The latter (with comma) is the presented way in PDSH\n",
    "    * This duality does not work for DataFrames\n",
    "* has row-then-column index order...\n",
    "    * ....with three rows in `[['l','i','s','t','1'],['s','c','n','d','2'],['t','h','r','d','3']]` \n",
    "* has slice by dimension as `start:stop:step` by default `0, len (this dimension), 1` \n",
    "    * ...exception: when `step` is negative `start` and `stop` are reversed\n",
    "    * ...multi-dimensional slices separated by commas\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-trade",
   "metadata": {},
   "source": [
    "## Time\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "There is time in association with data (when a sample was recorded) and time in association with\n",
    "code development (how long did this cell take to run?) Let's look at both.\n",
    "\n",
    "\n",
    "### Sample timing\n",
    "\n",
    "See PDSH-189. There are two time mechanisms in play: Python's built-in `datetime` and an improvement called\n",
    "`datetime64` from **numpy** that enables *arrays* of dates, i.e. time series. \n",
    "\n",
    "\n",
    "Consider these two ways of stipulating time slice arguments for `.sel()` applied to a DataSet.\n",
    "First:  Use a datetime64 with precision to minutes (or finer).\n",
    "Second: Pass strings that are interpreted as days, inclusive. In pseudo-code: \n",
    "\n",
    "```\n",
    "if do_precision:  \n",
    "   t0 = dt64('2019-06-01T00:00')\n",
    "   t1 = dt64('2019-06-01T05:20')\n",
    "   dss = ds.sel(time=slice(t0, t1))   \n",
    "else:\n",
    "    day1 = '24'\n",
    "    day2 = '27'              # will be 'day 27 inclusive' giving four days of results\n",
    "    dss = ds.sel(time=slice('2019-06-' + day1, '2019-08-' + day2))\n",
    "\n",
    "len(dss.time)\n",
    "```\n",
    "\n",
    "### Execution timing\n",
    "\n",
    "Time of execution in seconds: \n",
    "\n",
    "```\n",
    "from time import time\n",
    "\n",
    "toc = time()\n",
    "for i in range(12): j = i + 1\n",
    "tic = time()\n",
    "print(tic - toc)\n",
    "\n",
    "7.82012939453125e-05\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-sellers",
   "metadata": {},
   "source": [
    "## ipywidgets\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-toolbox",
   "metadata": {},
   "source": [
    "## Holoview\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-vintage",
   "metadata": {},
   "source": [
    "## Process these notes\n",
    "\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "\n",
    "\n",
    "### Open issues\n",
    "\n",
    "* Does re-sample to (say) minutes over a time where data are missing create skips? Or nans? \n",
    "* What does merge() do? \n",
    "\n",
    "```\n",
    "ds = ds.merge(ds_flort)\n",
    "```\n",
    "\n",
    "* Order: `.merge()` then `.resample()` with `mean()`; or vice versa? (existing code is vice-versa)\n",
    "    * This approach does resampling prior to merge but was taking way too long\n",
    "\n",
    "```\n",
    "ds = ds_flort.copy()\n",
    "ds = ds.reset_coords('seawater_pressure')        # converts the coordinate to a data variable\n",
    "ds_mean = ds.resample(time='1Min').mean()\n",
    "ds_std  = ds.resample(time='1Min').std()\n",
    "```\n",
    "\n",
    "* How to copy a dataset, move a coordinate to a data variable\n",
    "\n",
    "```\n",
    "ds = ds_ctdpf.copy()\n",
    "ds = ds.reset_coords('seawater_pressure')        # converts the coordinate to a data variable\n",
    "```\n",
    "\n",
    "* Standard deviation method\n",
    "\n",
    "```\n",
    "ds_std  = ds.resample(time='1Min').std()\n",
    "```\n",
    "\n",
    "* Load R/M Dataset ctdpf + flort\n",
    "    * Some data are noisier towards the surface, some are ridiculously noisy. \n",
    "    * Idea: Filter on standard deviation, threshold:\n",
    "        * Filter leaves a signal of interest? \n",
    "    * Fluorometer is particularly troublesome. \n",
    "\n",
    "\n",
    "* Depth profiles reduce to simple metrics\n",
    "    * profile start, peak, end times\n",
    "    * platform residence: start and end times (from profile times)\n",
    "    * (smoothed) chlorophyll derivative, curvature, rate of curvature\n",
    "    * Similarly salinity seems to go through a consistent double-zero in rate of curvature\n",
    "    * intersection depth as used in TDR; for example for temperature or salinity\n",
    "        * extrapolate smoothed pressure by backing off the derivative change\n",
    "        * extrapolate platform, intersect\n",
    "    * time of day / sun angle\n",
    "    * local time\n",
    "    * rate of ascent verify; 3m / minute?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turkish-wesley",
   "metadata": {},
   "source": [
    "## Spectrophotometer (SP) and Nitrate\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "The SP runs on ascent only, at about 3.7 samples per second. Compare nitrate that also runs \n",
    "on ascent only at about 3 samples per minute. Nitrate data is fairly straightforward; SP \n",
    "data is chaotic/messy. The objective is to reduce the SP to something interpretable.\n",
    "\n",
    "\n",
    "### Deconstructing data: process pattern\n",
    "\n",
    "\n",
    "- `ds = xr.open_dataset(fnm)` \n",
    "    - Data dispersed across files: Variant + wildcard: `ds = xr.open_mfdataset('data_with_*_.nc')`\n",
    "- `obs` dimensional coordinate creates degeneracy over multiple files\n",
    "    - Use `.swap_dims` to swap time for `obs`\n",
    "- `ds.time[0].values, ds.time[-1].values` gives a timespan but nothing about duty cycles\n",
    "    - 2019 spectrophotometer data at Oregon Slope Base: 86 channels, 7 million samples\n",
    "    - ...leading to...\n",
    "        - Only operates during midnight and noon ascent; at 3.7 samples per second\n",
    "        - Optical absorbance and beam attenuation are the two data types\n",
    "        - Data has frequent dropouts over calendar time\n",
    "        - Data has spikes that tend to register across all 86 channels\n",
    "        - Very poor documentation; even the SME report is cursory\n",
    "\n",
    "\n",
    "### [`optaa`](https://oceanobservatories.org/instrument-class/optaa/)\n",
    "\n",
    "\n",
    "SP **Oregon Slope Base** five years of monthly data availability.\n",
    "\n",
    "\n",
    "<BR>\n",
    "<img src=\"./images/spectrophotometer/osb_2019_sp_availability.png\" style=\"float: left;\" alt=\"drawing\" width=\"1200\"/>\n",
    "<div style=\"clear: left\"/>\n",
    "<BR>\n",
    "    \n",
    "    \n",
    "Courtesy the Interactive Oceans data portal. \n",
    "2019 is somewhat intermittent and stops in September.\n",
    "\n",
    "* Instrument duty cycle\n",
    "    * The SP on the Oregon Slope Base shallow profiler...\n",
    "        * runs episodically, twice per day: midnight and noon profile ascents (*not* descent)\n",
    "            * **First is 7:22 Zulu: midnight off the coast of Oregon**\n",
    "        * RCA shallow profilers execute nine profiles per day from 200m to 5m nominal depths\n",
    "        * Profile duration is time scale of an hour or two\n",
    "        * Here one profile is treated as a water column snapshot \n",
    "            * The operative dimension is pressure/depth, not time\n",
    "        * SP runs during *ascent*, nitrate measures on *descent*\n",
    "    * In between profiles: The profiler or Science Pod (SCIP) is at rest on a platform at a depth of 200 meters\n",
    "    * The ascent minimum depth is five meters but is typically more, varying with sea conditions\n",
    "* SP are optical absorption (abbreviated **OA**), beam attenuation (abbreviated **BA**), time and pressure\n",
    "    * Pressure in dbar and depth in meters are treated as equivalent\n",
    "    * Instrument sampling rate is ~3.7 samples per second\n",
    "    * Instrument records 86 spectral channels\n",
    "        * Light wavelength is ~(400nm + channel number x 4nm)\n",
    "        * Channel width is ~20nm so channels overlap\n",
    "        * Signals shift with wavelength making it possible to stack profiles in a single chart\n",
    "    * Channels 0, 83, 84 and 85 tend to give `nan` values (not usable) for both OA and BA\n",
    "        * In this work we tend to use channels 2 through 82\n",
    "    * Both OA and BA data are idiosyncratic\n",
    "        * The midnight OA data are quantized in a peculiar manner; see charts below\n",
    "        * The noon OA are *somewhat* quantized but have more reasonable / data-like structure\n",
    "        * BA data are not fraught with the OA quantization issue\n",
    "            * Both midnight and noon BA data include substantial noise\n",
    "            * Variance is also apparent in BA data\n",
    "            * This suggests filtering by depth bin and possibly discarding outliers\n",
    "* Un-answered questions\n",
    "    * Why are OA data different in midnight versus noon profiles? \n",
    "    * Are OA and BA typically combined into a turbidity value?\n",
    "    * What wavelength ranges are of particular interest?\n",
    "    * How do these signals compare with fluorometers, nitrate, CTD, pH, etcetera? \n",
    "        * The OOI site has a brief SME evaluation circa 2016 that does not illuminate actual data use cases\n",
    "* References froom OOI\n",
    "    * [Table of instruments / designators / locations](https://oceanobservatories.org/instrument-series/optaad/)\n",
    "    * [Spectrophotometer page](https://oceanobservatories.org/instrument-class/optaa/)\n",
    "    * [Subject Matter Expert evaluation](https://oceanobservatories.org/2016/07/successful-sme-evaluation-spectrophotometer-optaa/)\n",
    "    * [Code](https://github.com/oceanobservatories/ion-functions/blob/master/ion_functions/data/opt_functions.py)\n",
    "\n",
    "\n",
    "Paraphrasing the Subject Matter Export evaluation (link above): \n",
    "\n",
    "\n",
    "> Dr. Boss (SME) verified 1.5 months of data (April-May 2015): Processing and plotting data using the raw data and vendor calibration files \n",
    "> from the AC-S, salinity and temperature from a collocated CTD data to correct absorption and attenuation median spectra and scattering, \n",
    "> and data from a collocated fluorometer to cross-check the chlorophyll and POC results.\n",
    "> \n",
    "> Consistency between the sensors suggests that they did not foul during the deployment. Not only did his results show that accurate data \n",
    "> was being produced by all the sensors in question, but the AC-S (an extremely sensitive instrument normally deployed for very short periods\n",
    "> of time) did not drift noticeably during the deployment period, a notable achievement.\n",
    "\n",
    "\n",
    "\n",
    "From the [sheet on Optical Absorption (**OA**)](https://oceanobservatories.org/wp-content/uploads/2015/10/1341-00700_Data_Product_SPEC_OPTABSN_OOI.pdf):\n",
    "\n",
    "\n",
    "> The primary instrument (OPTAA) is the WET Labs ac-s spectral absorption and attenuation meter. \n",
    "The instrument provides a 75 wavelength output from approximately 400750 nm with approximately \n",
    "4 nm steps. Individual filter steps have a full-width half maximum response that\n",
    "range between about 10 to 18 nm. \n",
    ">\n",
    "> There are a total of 35 OPTAA instruments deployed\n",
    "throughout the initial OOI construction and integrated into the Pioneer, Endurance, Regional and\n",
    "Global arrays. They are deployed at fixed depths (near-surface, mid-water column and sea floor)\n",
    "and installed on moored profilers.\n",
    ">\n",
    "> The ac-s performs concurrent measurements of the water attenuation and absorption \n",
    "(the latter called 'OPTABSN').\n",
    ">\n",
    "> OPTABSN is a L2 product in that computation requires the raw signals emanating from a properly\n",
    "calibrated and configured instrument as well as water temperature (TEMPWAT) and practical\n",
    "salinity (PRACSAL) derived from a co-located and synchronized CTD. \n",
    ">\n",
    "> While small corrections\n",
    "for salinity are available at visible wavelengths (< 700 nm), temperature and salinity corrections\n",
    "are more significant at infrared wavelengths (> 700 nm) and must be performed on both the\n",
    "absorption and attenuation (OPBATTN) signals.\n",
    "\n",
    "\n",
    "The [beam attenuation (**BA**) sheet](https://oceanobservatories.org/wp-content/uploads/2015/10/1341-00690_Data_Product_SPEC_OPTATTN_OOI.pdf)\n",
    "is similar. Both give a mathematical basis for the data as well as (MATLAB?) code. \n",
    "\n",
    "\n",
    "### Practical interpretation of Spectrophotometer data\n",
    "    \n",
    "    \n",
    "The data are provided as NetCDF-CF format files. The code used here is the Python `XArray` package \n",
    "built to work with this file format, specifically via the `Dataset` and \n",
    "the `DataArray` structures. These in turn inherit from the `pandas` `DataFrame` and `Series`; which in turn\n",
    "are built on the `numpy` n-dimensional array `ndarray`. All of this requires considerable time to \n",
    "learn and internalize. Some care is taken in this narrative to provide salient remarks for the Learner. \n",
    "In polemical terms I recommend two approaches for the person new to this set of tools: \n",
    "    \n",
    "    \n",
    "- Read through this notebook without too much concern for detail (suited to a focus on \n",
    "    the data, not on learning to build new workflows).\n",
    "- Work methodically through Jake VanDerplas' excellent (free, online) book \n",
    "    [_The Python Data Science Handbook_ (herein abbreviated **PDSH**)](https://jakevdp.github.io/PythonDataScienceHandbook/) \n",
    "    particularly chapters 2 and 3, to gain expertise with `NumPy` and `pandas` prior to taking on `XArray`. \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "### Nitrate \n",
    "\n",
    "    \n",
    "This code follows suit the spectrophotometer. It is simpler because there is only a nitrate value \n",
    "and no wavelength channel. \n",
    "\n",
    "    \n",
    "I kept the pressure bins the same even though the nitrate averates about 3 three samples or less per minute\n",
    "during a 70 minute ascent. That's about three meters per minute so one sample per meter. Since the \n",
    "spectrophotometer bin depth is 0.25 meters there are necessarily a lot of empty bins (bins with no data)\n",
    "for the nitrate profile. \n",
    "\n",
    "    \n",
    "### Two open issues\n",
    "\n",
    "\n",
    "A curious artifact of the situation is from a past bias: I had understood that the SCIP makes pauses \n",
    "on descent to accommodate the nitrate sensor. I may be in error but now it looks like this sensor, \n",
    "the nitrate sensor, is observing on ascent which is continuous. This leaves open the question of \n",
    "why the pauses occur on the descent. If I have that right. \n",
    "\n",
    "\n",
    "Finally there are two nitrate signals: 'samp' and 'dark'. This code addresses only 'samp' as 'dark'\n",
    "is showing nothing of interest. So this is an open issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-invasion",
   "metadata": {},
   "source": [
    "##### to do \n",
    "\n",
    "- Add pco2\n",
    "- Write a data characterization report with simple questions answered\n",
    "    - The example I have in mind is 'does this sensor respond to ambient light?' and 'should it?'\n",
    "- Treat profiles as monolithic in time (philosophical); see dimension swapping code example below in this cell\n",
    "- set up running averages\n",
    "- smear / chooser\n",
    "- extend the use of time delimeters across everything that follows\n",
    "- get rid of the print...s and the other junk print\n",
    "- look at the nitrate doy usage and maybe shift to that\n",
    "- deal with \"at rest\": Is data still accumulated?\n",
    "- deal with platform coincident data: agrees with profiler?\n",
    "- Review the process of setting up all these refined source datasets...\n",
    "    - What if I don't *want* to look at January 2019\n",
    "    - What is the operational record of all the shallow profilers? \n",
    "- What is the shape of January? (for each sensor: What is the data \"box\"?)\n",
    "- Starting with pH let's consider colorized curtain plots as an important goal\n",
    "- Plot irradiance (7 options) versus PAR\n",
    "- Deal with two older images:\n",
    "    - ./images/misc/optaa_spectra_0_10_20_JAN_2019.png\n",
    "    - ./images/misc/nitrate_2019_JAN_1_to_10.png\n",
    "- Full review / integration of instrument notes into corresponding vis code blocks; see below this cell\n",
    "\n",
    "\n",
    "***Important: pH sensor fire once at the end of every profile; back in the platform***\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Instrumentation notes: Needs integration (see to do above)\n",
    "\n",
    "\n",
    "Manufacturer etc: [here](https://interactiveoceans.washington.edu/instruments/).\n",
    "\n",
    "\n",
    "A set of files loads as a single xarray *Dataset* comprised of multiple *DataArrays*.\n",
    "Writing out the Dataset gives DataArray names; but the DataArray can itself be invoked \n",
    "with `.attrs` to see additional attributes that are invisible when looking at Dataset \n",
    "attributes. This is useful for designing data simplification. \n",
    "\n",
    "```\n",
    "ds\n",
    "````\n",
    "\n",
    "...lists the DataArrays in the Dataset. \n",
    "\n",
    "```\n",
    "ds.density.attrs\n",
    "```\n",
    "\n",
    "...lists the attributes of the `density` DataArray.\n",
    "\n",
    "\n",
    "\n",
    "##### Optical absorption spectrophotometer\n",
    "\n",
    "\n",
    "* Seabird Scientific from acquisition of WETLABS: ac-s model\n",
    "* Devices mounted on the shallow profilers\n",
    "* 86 wavelengths per sample; in practice some nan values at both ends\n",
    "* Operates only during shallow profiler ascents\n",
    "  * Only on the two \"nitrate\" ascents each day\n",
    "  * Data sample is about one per 0.27 seconds\n",
    "  * However it often does a \"skip\" with a sample interval about 0.5 seconds\n",
    "  * The nitrate run ascent is ~62 minutes (ascent only); ~3 meters per minute\n",
    "  * Ascent is about 14,000 samples; so 220 samples per minute\n",
    "  * That is 70 samples per meter depth over 20 seconds\n",
    "* Per the User's Manual post-processing gets rather involved\n",
    "* Spectral absorption: parameter `a`, values typically 20 - 45. \n",
    "* Attenuation is `c` with values on 0 to 1.\n",
    "* Coordinates we want are `time`, `int_ctd_pressure`, `wavelength`\n",
    "  * `time` and `wavelength` are also dimensions\n",
    "* Data variables we want are `beam_attenuation` (this is `c`) and `optical_absorption` (`a`)\n",
    "* Per year data is about 1.7 billion floating point numbers\n",
    "  * 86 wavelengths x 2 (c, a) x 2 (ascent / day) x 14,000 (sample / ascent) x 365\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Photosynthetically Active Radiation (PAR)\n",
    "\n",
    "\n",
    "* Devices mounted on the shallow profiler and the SP platform\n",
    "* Seabird Scientific (from acquisition of Satlantic): PAR model\n",
    "* Some ambiguity in desired result: `par`, `par_measured` and `par_counts_output` are all present in the data file\n",
    "  * Since `qc` values are associated with it I will simply use `par_counts_output`\n",
    "\n",
    "\n",
    "##### Fluorometer\n",
    "\n",
    "\n",
    "* WETLABS (Seabird Scientific from acquisition) Triplet\n",
    "* Chlorophyll emission is at 683 nm\n",
    "* Measurement wavelengths in nm are 700.0 (scattering), 460.0 (cdom) and 695.0 (chlorophyll)\n",
    "* Candidate Data variables\n",
    "  * Definites are `fluorometric_chlorophyll_a` and `fluorometric_cdom`\n",
    "  * Possibles are `total_volume_scattering_coefficient`, `seawater_scattering_coefficient`, `optical_backscatter`\n",
    "    * qc points to total volume scattering and optical backscatter but I'll keep all three\n",
    "\n",
    "\n",
    "##### Nitrate (nutnr_a_sample and nutnr_a_dark_sample)\n",
    "\n",
    "\n",
    "##### pCO2 water (two streams: pco2w_b_sami_data_record and pco2w_a_sami_data_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-niagara",
   "metadata": {},
   "source": [
    "```\n",
    "####################\n",
    "#\n",
    "# Nitrate\n",
    "# \n",
    "#   dims:       time\n",
    "#   coords:     time and int_ctd_pressure\n",
    "#   data array: nitrate concentration\n",
    "#\n",
    "# To do\n",
    "#   identify when the data happens\n",
    "#   verify that the 'dark' means nothing...\n",
    "# \n",
    "####################\n",
    "\n",
    "ds_n03dark = xr.open_dataset(\"/data/rca/simpler/osb_sp_nutnr_a_dark_2019.nc\")\n",
    "ds_n03samp = xr.open_dataset(\"/data/rca/simpler/osb_sp_nutnr_a_sample_2019.nc\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "include_charts = False\n",
    "\n",
    "m_strs = ['01', '02', '03', '04', '05', '06', '07', '08', '09']           # relevant 2019 months\n",
    "m_days = [31, 28, 31, 30, 31, 30, 31, 31, 30]                             # days per month in 2019\n",
    "\n",
    "month_index = 0                                                           # manage time via months and days; 0 is January\n",
    "month_str   = m_strs[month_index]  \n",
    "year_str    = '2019'\n",
    "\n",
    "n_meters          = 200\n",
    "n_bins_per_meter  = 4\n",
    "halfbin           = (1/2) * (1/n_bins_per_meter)\n",
    "n_pressure_bins   = n_meters * n_bins_per_meter\n",
    "p_bounds          = np.linspace(0., n_meters, n_pressure_bins + 1)             # 801 bounds: 0., .25, ..., 200.                   \n",
    "pressure          = np.linspace(halfbin, n_meters - halfbin, n_pressure_bins)  # 800 centers: 0.125, ..., 199.875                  \n",
    "nc_upper_bound    = 40.\n",
    "\n",
    "ndays = m_days[month_index]\n",
    "ndayplots, dayplotdays = 10, list(range(10))\n",
    "\n",
    "l_da_nc_midn, l_da_nc_noon = [], []       # these lists accumulate DataArrays by day\n",
    "\n",
    "if include_charts:\n",
    "    fig_height, fig_width, fig_n_across, fig_n_down = 4, 4, 2, ndayplots\n",
    "    fig, axs = plt.subplots(ndayplots, fig_n_across, figsize=(fig_width * fig_n_across, fig_height * fig_n_down), tight_layout=True)\n",
    "\n",
    "for day_index in range(ndays):\n",
    "    \n",
    "    day_str  = day_of_month_to_string(day_index + 1); date_str = year_str + '-' + month_str + '-' + day_str\n",
    "    this_doy = doy(dt64(date_str))\n",
    "    clear_output(wait = True); print(\"on day\", day_str, 'i.e. doy', this_doy)\n",
    "    midn_start = date_str + 'T07:00:00'\n",
    "    midn_done  = date_str + 'T10:00:00'\n",
    "    noon_start = date_str + 'T20:00:00'\n",
    "    noon_done  = date_str + 'T23:00:00'\n",
    "\n",
    "    # pull out OA and BA for both midnight and noon ascents; and swap in pressure for time\n",
    "    ds_midn = ds_n03samp.sel(time=slice(dt64(midn_start), dt64(midn_done))).swap_dims({'time':'int_ctd_pressure'})\n",
    "    ds_noon = ds_n03samp.sel(time=slice(dt64(noon_start), dt64(noon_done))).swap_dims({'time':'int_ctd_pressure'})\n",
    "    \n",
    "    # print('pressures:', ds_midn.int_ctd_pressure.size, ds_noon.int_ctd_pressure.size, '; times:', ds_midn.time.size, ds_noon.time.size)    \n",
    "    midn = True if ds_midn.time.size > 0 else False\n",
    "    noon = True if ds_noon.time.size > 0 else False\n",
    "        \n",
    "    if midn:\n",
    "        da_nc_midn = ds_midn.nitrate_concentration.expand_dims({'doy':[this_doy]})\n",
    "        del da_nc_midn['time']\n",
    "        l_da_nc_midn.append(da_nc_midn.sortby('int_ctd_pressure').groupby_bins(\"int_ctd_pressure\", p_bounds, labels=pressure).mean().transpose('int_ctd_pressure_bins', 'doy'))\n",
    "        \n",
    "    if noon:\n",
    "        da_nc_noon = ds_noon.nitrate_concentration.expand_dims({'doy':[this_doy]})\n",
    "        del da_nc_noon['time']\n",
    "        l_da_nc_noon.append(da_nc_noon.sortby('int_ctd_pressure').groupby_bins(\"int_ctd_pressure\", p_bounds, labels=pressure).mean().transpose('int_ctd_pressure_bins', 'doy'))\n",
    "\n",
    "    if include_charts and day_index in dayplotdays:      # if this is a plotting day: Add to the chart repertoire\n",
    "\n",
    "        dayplotindex = dayplotdays.index(day_index) \n",
    "\n",
    "        if midn:\n",
    "            axs[dayplotindex][0].scatter(l_da_nc_midn[-1], pressure,  marker=',', s=2., color='r') \n",
    "            axs[dayplotindex][0].set(xlim = (.0, nc_upper_bound), ylim = (200., 0.), title='NC midnight')\n",
    "            axs[dayplotindex][0].scatter(ds_midn.nitrate_concentration, ds_midn.int_ctd_pressure, marker=',', s=1., color='b'); \n",
    "            \n",
    "        if noon:\n",
    "            axs[dayplotindex][1].scatter(l_da_nc_noon[-1], pressure,  marker=',', s=2., color='g')\n",
    "            axs[dayplotindex][1].set(xlim = (.0, nc_upper_bound), ylim = (200., 0.), title='NC noon')\n",
    "            axs[dayplotindex][1].scatter(ds_noon.nitrate_concentration, ds_noon.int_ctd_pressure, marker=',', s=1., color='k'); \n",
    "\n",
    "save_figure = False\n",
    "if save_figure: fig.savefig('/home/ubuntu/chlorophyll/images/misc/nitrate_2019_JAN_1_to_10.png')\n",
    "\n",
    "save_nitrate_profiles = False\n",
    "\n",
    "if save_nitrate_profiles: \n",
    "    ds_nc_midn = xr.concat(l_da_nc_midn, dim=\"doy\").to_dataset(name='nitrate_concentration')\n",
    "    ds_nc_noon = xr.concat(l_da_nc_noon, dim=\"doy\").to_dataset(name='nitrate_concentration')\n",
    "\n",
    "    ds_nc_midn.to_netcdf(\"/data1/nutnr/nc_midn_2019_01.nc\")\n",
    "    ds_nc_noon.to_netcdf(\"/data1/nutnr/nc_noon_2019_01.nc\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "super-pocket",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_166/2010963702.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# First set up the figure, the axis, and the plot element we want to animate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Animation in Python is one thing. Animation in a Jupyter notebook is another.\n",
    "# Animation in binder is yet another. Rather than try and bootstrap a lesson here\n",
    "# I present a sequence of annotated steps that create an animation, save it as \n",
    "# an .mp4 file, load it and run it: In a Jupyter notebook of course. Then we\n",
    "# will see how it does in binder.\n",
    "\n",
    "# At some point in working on this I did a conda install ffmpeg. I am not clear \n",
    "#   right now on whether this was necessary or not; I suspect not.\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# With [the inline] backend activated with this line magic matplotlib command, the output \n",
    "# of plotting commands is displayed inline within frontends like the Jupyter notebook, \n",
    "# directly below the code cell that produced it. The resulting plots will then also be stored \n",
    "# in the notebook document.\n",
    "\n",
    "# de rigeur, commented out here as this runs at the top of the notebook\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import animation, rc      # animation provides tools to build chart-based animations.\n",
    "                                          # Each time Matplotlib loads, it defines a runtime configuration (rc) \n",
    "                                          #   containing the default styles for every plot element you create. \n",
    "                                          #   This configuration can be adjusted at any time using \n",
    "                                          #   the plt. ... matplotlibrc file, which you can read about \n",
    "                                          #   in the Matplotlib documentation.\n",
    "\n",
    "\n",
    "from IPython.display import HTML, Video   # HTML is ...?...\n",
    "                                          # Video is used for load/playback\n",
    "\n",
    "# First set up the figure, the axis, and the plot element we want to animate\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_xlim(( 0, 2))\n",
    "ax.set_ylim((-2, 2))\n",
    "\n",
    "line, = ax.plot([], [], lw=2)\n",
    "\n",
    "# initialization function: plot the background of each frame\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return (line,)\n",
    "\n",
    "# animation function. This is called sequentially\n",
    "def animate(i):\n",
    "    x = np.linspace(0, 2, 1000)\n",
    "    y = np.sin(2 * np.pi * (x - 0.01 * i))\n",
    "    line.set_data(x, y)\n",
    "    return (line,)\n",
    "\n",
    "# call the animator. blit=True means only re-draw the parts that have changed.\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=12, blit=True)\n",
    "\n",
    "HTML(anim.to_html5_video())\n",
    "\n",
    "# print(anim._repr_html_() is None) will be True\n",
    "# anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_line(num, data, line):\n",
    "    line.set_data(data[..., :num])\n",
    "    return line,\n",
    "\n",
    "fig1 = plt.figure()\n",
    "\n",
    "data = .05 + 0.9*np.random.rand(2, 200)\n",
    "l, = plt.plot([], [], 'r-')                # l, takes the 1-tuple returned by plt.plot() and grabs that first \n",
    "                                           # and only element; so it de-tuples it\n",
    "\n",
    "plt.xlim(0, 1); plt.ylim(0, 1); plt.xlabel('x'); plt.title('test')\n",
    "\n",
    "lines_anim = animation.FuncAnimation(fig1, update_line, 200, fargs=(data, l), interval=1, blit=True)\n",
    "\n",
    "# fargs are additional arguments to 'update_line()' in addition to the frame number: data and line\n",
    "# interval is a time gap between frames (guess is milliseconds)\n",
    "# blit is the idea of modifying only pixels that change from one frame to the next\n",
    "\n",
    "# For direct display use this: HTML(line_ani.to_html5_video())\n",
    "lines_anim.save('./lines_tmp3.mp4')            # save the animation to a file\n",
    "Video(\"./lines_tmp3.mp4\")                      # One can add , embed=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-plumbing",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig2 = plt.figure()\n",
    "\n",
    "x = np.arange(-9, 10)\n",
    "y = np.arange(-9, 10).reshape(-1, 1)\n",
    "base = np.hypot(x, y)\n",
    "ims = []\n",
    "for add in np.arange(15):\n",
    "    ims.append((plt.pcolor(x, y, base + add, norm=plt.Normalize(0, 30)),))\n",
    "\n",
    "im_ani = animation.ArtistAnimation(fig2, ims, interval=50, repeat_delay=3000,\n",
    "                                   blit=True)\n",
    "# To save this second animation with some metadata, use the following command:\n",
    "# im_ani.save('im.mp4', metadata={'artist':'Guido'})\n",
    "\n",
    "HTML(im_ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-client",
   "metadata": {},
   "source": [
    "## Binder\n",
    "\n",
    "[Top](#Introduction)\n",
    "\n",
    "* Create a binder badge in the home page `README.md` of the repository. \n",
    "\n",
    "```\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/<accountname>/<reponame>/HEAD)\n",
    "\n",
    "```\n",
    "\n",
    "* In `<repo>/binder` create `environment.yml` to match the working environment\n",
    "    * For this repo as of 10/23/2021 `binder/environment.yml` was: \n",
    "\n",
    "\n",
    "```\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - matplotlib\n",
    "  - netcdf4\n",
    "  - xarray\n",
    "  - ffmpeg\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-money",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
